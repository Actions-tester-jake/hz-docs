= Backing Up Persisted Data
:description: You can take snapshots of your persistence store (persisted files) to be able to copy them onto other clusters without having to shut down your cluster. This process is called hot backup.

{description}

== Configuring Hot Backup

To create snapshots you must first configure the Persistence backup directory: `backup-dir`.

[tabs] 
==== 
XML:: 
+ 
-- 
[source,xml]
----
<hazelcast>
    ...
    <persistence enabled="true">
        <backup-dir>/mnt/hot-backup</backup-dir>
	...
    </persistence>
    ...
</hazelcast>
----
--

YAML::
+
--
[source,yaml]
----
hazelcast:
  persistence:
    enabled: true
    backup-dir: /mnt/hot-backup
----
--
Java::
+
--
[source,java]
----
PersistenceConfig PersistenceConfig = new PersistenceConfig();
PersistenceConfig.setBackupDir(new File("/mnt/hot-backup"));
...
config.setPersistenceConfig(PersistenceConfig);
----
--
====

== Using Hot Backup

Once configured, you can initiate a new backup via the Java API or from the Management Center.
The backup is started transactionally and cluster-wide. This means that either
all or none of the members start the same backup. The member which receives the backup
request determines a new backup sequence ID and sends that information to all members.
If all members respond that no other backup is currently in progress and that
no other backup request has already been made, then the coordinating member commands
the other members to start the actual backup process. This creates a directory under
the configured `backup-dir` with the name `backup-<backupSeq>` and start copying the
data from the original store.

The backup process is initiated nearly instantaneously on all members. Note that
since there is no limitation as to when the backup process is initiated, it may be
initiated during membership changes, partition table changes or during normal data update.
Some of these operations may not be completed fully yet, which means that some members
will backup some data while some members will backup a previous version of the same data.
This is usually solved by the anti-entropy mechanism on the new cluster which
reconciles different versions of the same data. Please check the
<<achieving-high-consistency-of-backup-data, Achieving High Consistency of Backup Data section>>
for more information.

The duration of the backup process and the disk data usage drastically depends on
what is supported by the system and the configuration. Please check the
<<achieving-high-performance-of-backup-process, Achieving high performance of backup process section>>
for more information on achieving better resource usage of the backup process.

Following is an example of how to trigger the Hot Backup via API:

[source,java]
----
HotRestartService service = instance.getCluster().getHotRestartService();
service.backup();
----

The `backupSeq` is generated by the hot backup process, but you can define
your own backup sequences as shown below:

[source,java]
----
HotRestartService service = instance.getCluster().getHotRestartService();
long backupSeq = ...
service.backup(backupSeq);
----

Keep in mind that the backup fails if any member contains a backup directory
with the name `backup-<backupSeq>`, where `backupSeq` is the given sequence.

== Starting the Cluster From a Backup

The backup process creates sequenced subfolders
named `backup-<backupSeq>` in the configured <<configuring-hot-backup, hot backup folder>>
(`backup-dir`). To start a cluster with data from a specific backup, you need to set
the <<global-persistence-configuration, base folder>> (`base-dir`) to the desired backup subfolder.

For example, if you configure your cluster members with the following: 

```
base-dir=/opt/hz/data/
backup-dir=/opt/hz/backups
```

You would copy each existing member’s specific backup subfolder to the respective new member’s base-dir. So, assuming the new members also had the same configuration `base-dir` and `backup-dir`, you would copy `/opt/hz/backups/backup-<backupSeq>/*` from the existing member to `/opt/hz/data` on the new member.

== Achieving High Consistency of Backup Data

The backup is initiated nearly simultaneously on all members but you can
encounter some inconsistencies in the data. This is because some members might have
and some might not have received updated values yet from executed operations,
because the system could be undergoing partition and membership changes or
because there are some transactions which have not yet been committed.

To achieve a high consistency of data on all members, the cluster should be
put to `PASSIVE` state for the duration of the call to the backup method.
See the xref:management:cluster-utilities.adoc#cluster-member-states[Cluster Member States section] on information on how to do this.
The cluster does not need to be in `PASSIVE` state for the entire
duration of the backup process, though. Because of the design, only partition metadata
is copied synchronously during the invocation of the backup method. Once the backup method has returned,
all cluster metadata is copied and the exact partition data which needs to be copied is marked.
After that, the backup process continues asynchronously and you can return the cluster to the
`ACTIVE` state and resume operations.

== Achieving High Performance of Backup Process

Because of the design of persistence store, we can use hard links to achieve
backups/snapshots of the store. The backup process uses hard links
whenever possible because they provide big performance benefits and because
the backups share disk usage.

The performance benefit comes from the fact that Persistence file contents are
not being duplicated (thus using disk and I/O resources) but rather a new file name
is created for the same contents on disk (another pointer to the same inode).
Since all backups and stores share the same inode, disk usage drops.

The bigger the percentage of stable data in the persistence store
(data not undergoing changes), the more files each backup shares with the operational
persistence store and the less disk space it uses. For the backup to use hard links,
you must be running Hazelcast members on JDK 7 or higher and must satisfy all requirements for the
link:https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#createLink(java.nio.file.Path,%20java.nio.file.Path)[Files.createLink() method^] to be supported.

The backup process initially attempts to create a new hard link and
if that fails for any reason it continues by copying the data.
Subsequent backups also attempt to use hard links.

== Backup Process Progress and Completion

Only cluster and distributed object metadata is copied synchronously
during the invocation of the backup method. The rest of the
persistence store containing partition data is copied asynchronously
after the method call has ended. You can track the progress by API or
view it from the Management Center.

An example of how to track the progress via API is shown below:

[source,java]
----
HotRestartService service = instance.getCluster().getHotRestartService();
BackupTaskStatus status = service.getBackupTaskStatus();
...
----

The returned object contains the local member's backup status:

* the backup state (NOT_STARTED, IN_PROGRESS, FAILURE, SUCCESS)
* the completed count
* the total count

The completed and total count can provide you a way to track the
percentage of the copied data. Currently the count defines the
number of copied and total local member persistence stores
(defined by `PersistenceConfig.setParallelism()`)
but this can change at a later point to provide greater resolution.

Besides tracking the Persistence status by API, you can view the status in the
Management Center and you can inspect the on-disk files for each member.
Each member creates an `inprogress` file which is created in each of the copied persistence stores.
This means that the backup is currently in progress. When the backup task completes
the backup operation, this file is removed. If an error occurs during the backup task,
the `inprogress` file is renamed to `failure` which contains a stack trace of the exception.

== Interrupting and Canceling a Backup

Once the backup method call has returned and asynchronous copying of the
partition data has started, the backup task can be interrupted.
This is helpful in situations where the backup task has started at an inconvenient time.
For instance, the backup task could be automatized and it could be accidentally triggered
during high load on the Hazelcast instances, causing the performance of the Hazelcast instances to drop.

The backup task mainly uses disk IO, consumes little CPU and it generally
does not last for a long time (although you should test it with your environment
to determine the exact impact). Nevertheless, you can abort the backup tasks
on all members via a cluster-wide interrupt operation.
This operation can be triggered programmatically or from the Management Center.

An example of programmatic interruption is shown below:

[source,java]
----
HotRestartService service = instance.getCluster().getHotRestartService();
service.interruptBackupTask();
...
----

This method sends an interrupt to all members.
The interrupt is ignored if the backup task is currently not in progress
so you can safely call this method even though it has previously been
called or when some members have already completed their local backup tasks.

You can also interrupt the local member backup task as shown below:

[source,java]
----
HotRestartService service = instance.getCluster().getHotRestartService();
service.interruptLocalBackupTask();
...
----

The backup task stops as soon as possible and it does not remove the
disk contents of the backup directory meaning that you must remove it manually.

