= Configuring How Clusters Handle Single-Member Failures
:description: When cluster members have persisted data on disk, the way in which the cluster handles a single member failure depends on how you configure the cluster to deal with invalid partitions or slow migrations.
:page-enterprise: true

{description}

== How Clusters Handle Single-Member Failures

By default, if a cluster detects that a member is restarting and attempting to rejoin, the clusterâ€™s master member will ask the member to send its partition table for validation.

If the master member validates that the partition table was correct at the time the member left the cluster, the member loads its persisted data from disk.

If the cluster <<handling-invalid-partitions, cannot validate the member's partition table>> for whatever reason, the member stops trying to join the cluster and shuts down. The cluster will not allow the member to rejoin until it has deleted its persistence store. The cluster then recovers the missing member's data from backups and
redistributes it among the remaining cluster members.

== Handling Invalid Partitions

To handle cases where the master member rejects a rejoining member because of invalid partitions, you can do one of the following:

- Set the xref:configuring-persistence.adoc#persistence.auto-remove-stale-data[`auto-remove-stale-data` configuration option] to true.
+
NOTE: A joining member that crashes while in an `ACTIVE` state will generally not consider its data stale. Instead, the member will reload its data from disk and then synchronize it with other cluster members as needed. Only in some exceptional circumstances will the master member consider the joining member's data stale.
- <<force-start, Use Force Start>> to delete the data and restart the member.
- <<partial-start, Use Partial Start>> to configure a strategy for this scenario.

=== Force Start

A member can crash permanently and then be unable to recover from the failure.
In this case, you can force the cluster
to clean its persisted data and make a fresh start in a process called _force start_.

WARNING: Force start is a destructive process, which results
in the deletion of persisted data.

Assume the following which is a valid scenario to use force start:

* You have a cluster consisting of members A and B which is initially stable.
* Cluster transitions into `FROZEN` or `PASSIVE` state.
* Cluster gracefully shuts down.
* Member A restarts, but member B does not.
* Member A waits for member B to join, which never happens.
* Now you have the choice to Force Start the cluster without member B.
* Cluster discards all data and starts empty.

To trigger a force start use one of the following options:

- xref:{page-latest-supported-mc}@management-center:monitor-imdg:cluster-administration.adoc#hot-restart[Management Center]

- xref:management:cluster-utilities.adoc#partial-start-force-start[REST API]

- xref:management:cluster-utilities.adoc#example-usages-for-cluster-sh[Cluster management scripts]

=== Partial Start

Partial start allows a cluster to start with an incomplete member set.
Data belonging to those missing members is assumed lost and Hazelcast tries
to recover missing data using the restored backups. For example, if you have a
minimum of two backups configured for all maps, then having at most two missing members will be safe against data loss. If there are more
than two missing members or there are maps with less than two backups,
then expect data loss.

To enable partial start, configure one of the following restart strategies in the xref:configuring-persistence.adoc#persistence.cluster-data-recovery-policy[`cluster-data-recovery-policy`
option]:

- `PARTIAL_RECOVERY_MOST_RECENT`
- `PARTIAL_RECOVERY_MOST_COMPLETE`

When partial start is enabled, Hazelcast can perform a partial start
automatically or manually, in cases where some members are unable to restart
successfully.

Partial start proceeds automatically when some members fail to
start and join to the cluster within `validation-timeout-seconds`. After the
`validation-timeout-seconds` duration is passed, Hazelcast chooses to perform
partial start with the members present in the cluster.

To trigger a manual partial start, use one of the following options before the `validation-timeout-seconds` duration expires:

- xref:{page-latest-supported-mc}@management-center:monitor-imdg:cluster-administration.adoc#hot-restart[Management Center]

- xref:management:cluster-utilities.adoc#partial-start-force-start[REST API]

- xref:management:cluster-utilities.adoc#example-usages-for-cluster-sh[Cluster management scripts]

The other situation to decide to perform a partial start is failures during
the data load phase. When Hazelcast learns data load result of all members which
have passed the validation step, it automatically performs a partial start with
the ones which have successfully restored their data. Please note that
partial start does not expect every member to succeed in the data load step.
It completes the process when it learns data load result for every member and
there is at least one member which has successfully restored its data.
Relatedly, if it cannot learn data load result of all members before `data-load-timeout-seconds`
duration, it proceeds with the ones which have already completed the data load process.

Selection of members to perform partial start among live members is done
according to the `cluster-data-recovery-policy` configuration.
Set of members which are not selected by the `cluster-data-recovery-policy`
are called `Excluded members` and they are instructed to perform <<force-start, force start>>.
Excluded members are allowed to join cluster only when they clean their
data and make a fresh-new start. This is a completely automatic
process. For instance, if you start the missing members after partial start
is completed, they clean their data and join the cluster.

Please note that partial start is a destructive process. Once it is completed,
it cannot be repeated with a new configuration. For this reason, one may need
to perform the partial start process manually. Automatic behavior of partial start
relies on `validation-timeout-seconds` and `data-load-timeout-seconds` configuration
values. If you need to control the process manually, `validation-timeout-seconds` and
`data-load-timeout-seconds` properties can be set to very big values so that
Hazelcast cannot make progress on timeouts automatically. Then, the overall
process can be managed manually via aforementioned methods, i.e.,
Management Center, REST API and cluster management scripts.

== Handling Slow Migrations

While a cluster is migrating a member's partitions, the performance of the cluster may be affected by reduced throughput or temporary unavailability of data.

If you have lots of persisted data and you are concerned about how long it may take for your cluster to migrate data after a member fails to rejoin, you can configure the following options:

- <<delaying-migrations, Delay migrations>>.
+
WARNING: Do not use this option if your cluster also stores in-memory data. This option stops the cluster from migrating in-memory data. As a result any data that is not persisted will be lost if the member restarts, including backups.

- <<synchronzing-persisted-data-faster, Synchronize persisted data faster>> while also allowing the cluster to continue migrations as usual.

=== Delaying Migrations

Delaying migrations stops your cluster from migrating a failed member's data too soon. This option is useful if you have lots of persisted data that would take too long to migrate, and you want to give members more time to restart.

You may want to set a rebalance delay when you expect members to shut down and restart quickly such as in the following scenarios:

- You're carrying out a planned shutdown.
- You're running a cluster on Kubernetes and expect members to be restarted quickly.

To delay migrations during a single member failure, configure a _rebalance delay_, using the xref:ROOT:system-properties.adoc#rebalance-delay[`hazelcast.partition.rebalance.after.member.left.delay.seconds` property].

WARNING: Do not use configure this propery if your cluster also stores in-memory data. A non-zero value will stop the cluster from migrating in-memory data. As a result any data that is not persisted will be lost if the member restarts, including backups. 

Assume the following:

* A cluster consists of members A, B, and C with Persistence enabled.
* Member B is killed.
* Member B restarts.

If member B restarts within the rebalance delay, all its persisted data will be restored from disk and no migrations will take place.

While the member is down, operations on partitions that are owned by that member will be retried until they either time out or the member restarts and executes the requested operation. As a result, this option is best when you prefer a latency spike rather than migrating data over the network.

If member B does not restart within the rebalance delay, the cluster recovers member B's data from backups and
redistributes the data among the remaining members (members A and C
in this case). If member B is later restarted, it recovers its persisted data from disk and brings it up-to-date with data from members A and C. If Merkle trees are enabled on available data structures, migrations use those to request only missing persisted data. For details about how members use Merkle trees, see <<synchronizing-persisted-data-faster, Synchronizing Persisted Data Faster>>.

=== Synchronizing Persisted Data Faster

When a failed member rejoins the cluster, it populates its in-memory stores with data from disk that may be stale.

If you have lots of persisted data as well as in-memory data that you don't want to lose, you can configure your data structures to generate a Merkle tree.

The Merkle tree stores the state of persisted data in a way that other cluster members can quickly read, compare with their own, and check the delta for what is missing. This way, after a restart, the member can send its Merkle tree to the cluster and request only the missing data, reducing the amount of data sent over the network.

On map and JCache data structures, you can configure the following options to enable the Merkle tree.

[cols="1a,1a,1m,2a",options="header"]
|===
|Option|Description|Default|Example

|`merkle-tree.enabled`
|Whether a Merkle tree is generated for the data structure.
|disabled
|
[tabs] 
==== 
XML:: 
+ 
--
[source,xml]
----
<hazelcast>
  <map name="test-map">
    <persistence enabled="true">
    </persistence>
    <merkle-tree enabled="true">
    </merkle-tree>
  </map>
</hazelcast>
----
--
YAML:: 
+ 
--
[source,yaml]
----
hazelcast:
  map:
  test-map:
    persistence:
      enabled: true
    merkle-tree:
      enabled: true
----
--
Java:: 
+ 
--
[source,java]
----
Config config = new Config();

MapConfig mapConfig = config.getMapConfig("test-map");
mapConfig.getDataPersistenceConfig().setEnabled(true)
mapConfig.getMerkleTreeConfig().setEnabled(true);

config.addMapConfig(mapConfig);
----
--
====

|`merkle-tree.depth`
|The depth of the Merkle tree.

The deeper the tree, the more accurate the difference detection and the more space is needed to store the Merkle tree in memory.
|10
|
[tabs] 
==== 
XML:: 
+ 
--
[source,xml]
----
<hazelcast>
  <map name="test-map">
    <persistence enabled="true">
    </persistence>
    <merkle-tree enabled="true">
      <depth>
        12
      </depth>
    </merkle-tree>
  </map>
</hazelcast>
----
--
YAML:: 
+ 
--
[source,yaml]
----
hazelcast:
  map:
  test-map:
    persistence:
      enabled: true
    merkle-tree:
      enabled: true
      depth: 12
----
--
Java:: 
+ 
--
[source,java]
----
Config config = new Config();

MapConfig mapConfig = config.getMapConfig("test-map");
mapConfig.getDataPersistenceConfig().setEnabled(true)
mapConfig.getMerkleTreeConfig().setEnabled(true);
mapConfig.getMerkleTreeConfig().setDepth(12);

config.addMapConfig(mapConfig);
----
--
====
|===