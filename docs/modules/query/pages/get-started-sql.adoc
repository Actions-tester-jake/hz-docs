= Get Started with SQL Queries
:description: In this tutorial, you learn the basics of querying in SQL by using the interactive SQL shell on your Hazelcast member to run ad-hoc, batch, federated, and streaming queries.

{description}

== Before You Begin

To complete this tutorial, you need the following:

[cols="1a,1a"]
|===
|Prerequisites|Useful resources

|A Hazelcast cluster in client/server mode and an instance of Management Center running on your local network.
|xref:getting-started:get-started-binary.adoc[Start a Local Cluster].

|Enable the Jet engine.
|The Jet engine is needed to run SQL queries. For information on how to enable the Jet engine, see xref:pipelines:job-security.adoc[].
|===

[.interactive-button]
xref:interactive-sql.adoc[Try the interactive example,window=_blank]

== Step 1. Create a Data Source to Query

SQL can query data in maps, Kafka topics and the local cluster's file system.

In this step, you create a `.csv` file that you can use to query.

. Create a file named `likes.csv`.

. Add the following data to your file.
+
[source,shell]
----
id,name,likes
1,Jerry,13
2,Greg,108
3,Mary,73
4,Jerry,88
----
+
This example file contains a record of the number of likes each person has.

== Step 2. Connect to the SQL Shell on your Cluster Member

The easiest way to run SQL queries on a cluster is to connect to the SQL shell.

[tabs]
====
Binary::
+
--
.Mac and Linux
[source,shell]
----
bin/hz-cli sql
----

.Windows
[source,shell]
----
bin/hz-cli.bat sql
----
--
Docker::
+
--
Replace the `$LOCAL_IP` placeholder with your member's local IP address.

[source,shell,subs="attributes+"]
----
docker run --network hazelcast-network -it --rm hazelcast/hazelcast:{page-component-version} hz-cli --targets hello-world@$LOCAL_IP sql
----

The `--targets` parameter tells the SQL shell to connect to the member at the given IP address in a cluster called `hello-world`.

TIP: Make sure you mount the `likes.csv` file on the container that's running your member.
--
====

== Step 3. Configure the File Connector

To allow Hazelcast to find and recognize the data in your `.csv` file, you need to configure the xref:integrate:file-connector.adoc[file connector].

Use the xref:sql:create-mapping.adoc[`CREATE MAPPING` statement] to configure the file connector and give Hazelcast access to the data in the `likes.csv` file.

[source,sql]
----
CREATE MAPPING csv_likes (id INT, name VARCHAR, likes INT)
TYPE File
OPTIONS ('format'='csv',
    'path'='/absolute/path/to/current/folder', 'glob'='likes.csv');
----

TIP: Make sure you replace the `path` option with the absolute path to your `.csv` file.

== Step 4. Run Ad-Hoc Queries

Ad-hoc queries allow you to retrieve a small subset of data. Usually these queries are simple and you can have many of them running concurrently in a Hazelcast cluster.

A common use case for ad-hoc queries is individual business transactions where you need to get or update data.

. Use a xref:sql:select.adoc[`SELECT` statement] to query all the data in the `likes.csv` file.
+
[source,sql]
----
SELECT * FROM csv_likes;
----
+
You should see the following:
+
[source,shell]
----
+------------+--------------------+------------+
|          id|name                |       likes|
+------------+--------------------+------------+
|           1|Jerry               |          13|
|           2|Greg                |         108|
|           3|Mary                |          73|
|           4|Jerry               |          88|
+------------+--------------------+------------+
----

. Query only the `name` and `likes` columns, by adding them as a comma-separated list after the `SELECT` statement.
+
[source,sql]
----
SELECT name, likes FROM csv_likes;
----
+
```
+--------------------+------------+
|name                |       likes|
+--------------------+------------+
|Jerry               |          13|
|Greg                |         108|
|Mary                |          73|
|Jerry               |          88|
+--------------------+------------+
```

. Use a filter to display only the names of people with more than 20 likes.
+
[source,sql]
----
SELECT name FROM csv_likes WHERE likes > 20;
----
+
```
+--------------------+
|name                |
+--------------------+
|Greg                |
|Mary                |
|Jerry               |
+--------------------+
```

. Give the `name` column an alias for the query results.
+
NOTE: This clause does not rename the column in the table.
+
[source,sql]
----
SELECT name AS popular_users, likes
FROM csv_likes
WHERE likes > 20;
----
+
```
+--------------------+------------+
|popular_users       |       likes|
+--------------------+------------+
|Greg                |         108|
|Mary                |          73|
|Jerry               |          88|
+--------------------+------------+
```

. To filter rows on more than one condition, you can join conditions with the `AND`, `OR`, and `NOT` operators.
+
[source,sql]
----
SELECT *
FROM csv_likes
WHERE likes > 20 AND name = 'Mary';
----
+
```
+------------+--------------------+------------+
|          id|name                |       likes|
+------------+--------------------+------------+
|           3|Mary                |          73|
+------------+--------------------+------------+
```

== Step 5. Run Batch Queries

Batch queries allow you to query large datasets either in one or multiple systems and/or run aggregations on them to get deeper insights. Usually these queries are complex and you can run a small number of them concurrently in a Hazelcast cluster.

Common uses of OLAP include business reporting functions such as financial analysis, budgeting, and forecast planning.

. Use the `SUM()` function to aggregate the total number of likes for each person and group the results by name.
+
[source,sql]
----
SELECT name, sum(likes) AS total_likes FROM csv_likes GROUP BY name;
----
+  
You should see the following:
+
```
+--------------------+--------------------+
|name                |         total_likes|
+--------------------+--------------------+
|Greg                |                 108|
|Jerry               |                 101|
|Mary                |                  73|
+--------------------+--------------------+
```
+
The results do not include a row for each Jerry because the `GROUP BY` statement groups the results by name.

. Filter for the names that have more than 100 likes combined, using the `HAVING` clause. This clause is equivalent to the `WHERE` clause but for aggregate groups.
+
[source,sql]
----
SELECT name AS most_liked
FROM csv_likes
GROUP BY name HAVING SUM(likes) > 100;
----
+
```
+--------------------+
|most_liked          |
+--------------------+
|Jerry               |
|Greg                |
+--------------------+
```

For a list of available aggregations, see xref:sql:expressions.adoc[].

If you need more control over how your data is being transformed and aggregated, you may want to xref:pipelines:overview.adoc[build a pipeline with the Jet API].

== Step 6. Run Federated Queries

Federated queries are those that join tables from different datasets.

Normally, querying in SQL is database or dataset-specific. However, with xref:pipelines:sources-sinks[connectors], you can pull information from different sources and present a more complete picture of the data.

. Configure the map connector to create a new table called `dislikes`.
+
[source,sql]
----
CREATE MAPPING dislikes (
name VARCHAR,
dislikes INT
) TYPE IMap OPTIONS ('keyFormat'='int', 'valueFormat'='json');
----
+
This table is mapped to a distributed map in Hazelcast where the key is an integer and the value is an object that's serialized to JSON.

. Use `SINK INTO` statements to add some entries to the map.
+
[source,sql]
----
SINK INTO dislikes VALUES(1, 'Greg', 1);
SINK INTO dislikes VALUES(2, 'Jerry', 0);
SINK INTO dislikes VALUES(3, 'Mary', 5);
SINK INTO dislikes VALUES(4, 'Jerry', 0);
----

. Use the xref:sql:select.adoc#join-tables[`JOIN` clause] to merge results from the `likes` and `dislikes` tables so you can see who has the most likes and dislikes.
+
NOTE: The data source on the right of the join must always be a map.
+
[source,sql]
---- 
SELECT csv_likes.name, csv_likes.likes, dislikes.dislikes
FROM csv_likes
JOIN dislikes
ON csv_likes.id = dislikes.__key;
----
+
```
+--------------------+------------+------------+
|name                |       likes|    dislikes|
+--------------------+------------+------------+
|Jerry               |          13|           0|
|Greg                |         108|           5|
|Mary                |          73|           5|
|Jerry               |          88|          20|
+--------------------+------------+------------+
```

. Use the `ORDER BY` clause to order the results by name and use the `LIMIT` clause to limit them so that only the first two are displayed.

[source,sql]
----
SELECT csv_likes.name, csv_likes.likes, dislikes.dislikes
FROM csv_likes
JOIN dislikes
ON csv_likes.id = dislikes.__key
ORDER BY csv_likes.name
LIMIT 2;
----

== Step 7. Run Streaming Queries

Streaming queries are those that continuously produce results. These queries get data from streaming sources. At the moment, the only supported streaming source for SQL are the xref:integrate:kafka-connector.adoc[Kafka connector] or the xref:sql:expressions.adoc#table-valued-functions[`TABLE(generate_stream())` table-valued function].

To run a streaming query on Kafka, you need to configure the connector with the `CREATE MAPPING` statement.

But for simplicity, se the `TABLE(generate_stream())` function to test what happens when you run a streaming query.

[source,sql]
----
SELECT * FROM TABLE(generate_stream(100)) WHERE v / 10 * 10 = v;
----

This query generates an infinite stream of numbers and filters the results so that only multiples of 100 are returned.

In SQL, a stream is like a table with infinite rows that you can only access sequentially and thus never reach the end. As a result, you will get an error if you try to aggregate the stream without using a window function. SQL does not yet support window functions. To aggregate results of streaming queries, use the Jet API.

TIP: This example relies on the client being connected to the cluster to receive the results of the query. To run this query in the background and send the results to a sink, see the tutorial on building a data pipeline with a streaming query, see xref:pipelines:learn-sql.adoc[].

== Next Steps

Learn how to xref:querying-maps-sql.adoc[query maps with SQL].

Explore xref:sql:sql-statements.adoc[all available SQL statements].