= Get Started with SQL Over Kafka
:description: Use an interactive SQL shell on your Hazelcast member to query real-time streaming data as it is being generated and ingest the results into a map.
:page-aliases: pipelines:learn-sql.adoc

{description}

== Before You Begin

To complete this tutorial, you need the following:

[cols="1a,1a"]
|===
|Prerequisites|Useful resources

|A Hazelcast cluster in client/server mode and an instance of Management Center running on your local network 
|xref:getting-started:get-started-binary.adoc[Start a Local Cluster]

|Enable the Jet engine.
|The Jet engine is needed to run SQL queries. For information on how to enable the Jet engine, see xref:pipelines:job-security.adoc[].
|===

[.interactive-button]
xref:interactive-sql-kafka.adoc[Try the interactive example,window=_blank]

== Step 1. Query Streaming Data from Apache Kafka

In this step, you use SQL to query a Kafka topic and display the results in the shell.

. Connect to the SQL shell on your cluster member.
+
[tabs]
====
Binary::
+
--
.Mac and Linux
[source,shell]
----
bin/hz-cli sql
----

.Windows
[source,shell]
----
bin/hz-cli.bat sql
----
--
Docker::
+
--
Replace the `$LOCAL_IP` placeholder with your member's local IP address.

[source,shell,subs="attributes+"]
----
docker run --network hazelcast-network -it --rm hazelcast/hazelcast:{page-component-version} hazelcast --targets hello-world@$LOCAL_IP sql
----

The `--targets` parameter tells the SQL shell to connect to the member at the given IP address in a cluster called `hello-world`.

TIP: Make sure you mount the `likes.csv` file on the container that's running your member.
--
====

. On the same device as your Hazelcast member, start a Kafka server.
+
[tabs]
====
Binary::
+
--
. Download Kafka.
+
[source,shell]
----
wget http://mirror.cc.columbia.edu/pub/software/apache/kafka/2.7.0/kafka_2.13-2.7.0.tgz
tar xvf kafka_2.13-2.7.0.tgz
cd kafka_2.13-2.7.0
----

. Start Zookeeper.
+
[source,shell]
----
bin/zookeeper-server-start.sh config/zookeeper.properties
----

. In another terminal, start Kafka.
+
[source,shell]
----
bin/kafka-server-start.sh config/server.properties 
----
--
Docker::
+
--
[source,shell]
----
docker run --name kafka --network hazelcast-network --rm hazelcast/hazelcast-quickstart-kafka
----
--
====

. In the SQL shell, create a mapping to Kafka to allow Hazelcast to access data that is pushed to the Kafka server.
+
Here, you configure the connector to read trading information as JSON messages with the following fields:
+
[source,json]
----
{
  "id"
  "ticker"
  "price"
  "amount"
}
----
+
[tabs]
====
Binary::
+
--
[source,sql]
----
CREATE MAPPING trades (
    id BIGINT,
    ticker VARCHAR,
    price DECIMAL,
    amount BIGINT)
TYPE Kafka
OPTIONS (
    'valueFormat' = 'json',
    'bootstrap.servers' = '127.0.0.1:9092'
);
----
--
Docker::
+
--
[source,sql]
----
CREATE MAPPING trades (
    id BIGINT,
    ticker VARCHAR,
    price DECIMAL,
    amount BIGINT)
TYPE Kafka
OPTIONS (
    'valueFormat' = 'json',
    'bootstrap.servers' = 'kafka:9092'
);
----
--
====

. Write a streaming query that filters trade events from Kafka.
+
[source,sql]
----
SELECT ticker, ROUND(price * 100) AS price_cents, amount
  FROM trades
  WHERE price * amount > 100;
----
+
You should see an empty table:
+
```
+------------+----------------------+-------------------+
|ticker      |           price_cents|             amount|
+------------+----------------------+-------------------+
```
+
NOTE: Streaming queries like this one continue to run until you close the shell or kill the process with **Ctrl** + **C**.

. In another terminal, open another connection to the SQL shell and push some messages to Kafka.
+
[source,sql]
----
INSERT INTO trades VALUES
  (1, 'ABCD', 5.5, 10),
  (2, 'EFGH', 14, 20);
----

. Go back to the terminal where you created the streaming query.
+
You should see that Hazelcast has executed the query and filtered the results:
+
```
+-----------------+----------------------+-------------------+
|ticker           |           price_cents|             amount|
+-----------------+----------------------+-------------------+
|EFGH             |                  1400|                 20|
```

== Step 2. Ingest Query Results into a Hazelcast Map

To save your Kafka query results as a view that you can later access faster, you can cache them in Hazelcast by ingesting them into a map.

Because Kafka is a streaming source, your query continues to run until it is canceled and results are returned to the SQL shell as soon as they are ready. Therefore, to ingest data, you need to set up a _job_ to allow the streaming query to run in the background and insert the results into a map.

. Create a mapping to a new map in which to ingest your streaming query results.
+
```sql
CREATE MAPPING tradeMap (
__key BIGINT,
ticker VARCHAR,
price DECIMAL,
amount BIGINT)
TYPE IMap
OPTIONS (
'keyFormat'='bigint',
'valueFormat'='json-flat');
```

. Submit a streaming job to your cluster that will monitor your Kafka topic (`SELECT id, ticker, price, amount FROM trades`) for changes and store them in a map (`SINK INTO tradeMap`).
+
```sql
CREATE JOB ingest_trades AS
SINK INTO tradeMap
SELECT id, ticker, price, amount
FROM trades;
```
+
NOTE: A streaming job will run indefinitely until it is explicitly canceled or the cluster is shut down. Even if you kill the shell connection, the job will continue running on the cluster.

. List your job to make sure that it was successfully submitted.
+
```sql
SHOW JOBS;
```
+
You should see a job called `ingest_trades`.
+
```
+--------------------+
|name                |
+--------------------+
|ingest_trades       |
+--------------------+
```

. Publish some events to the Kafka topic.
+
```sql
INSERT INTO trades VALUES
(1, 'ABCD', 5.5, 10),
(2, 'EFGH', 14, 20);
```

. Query your `tradeMap` map to see that the Kafka data has been added to it.
+
```sql
SELECT * FROM tradeMap;
```
+
You should see that the data coming from Kafka is being stored in your map.
+
```
+---------+---------+----------+------------+
|       id|ticker   |     price|      amount|
+---------+---------+----------+------------+
|        2|EFGH     |14.000000…|          20|
|        1|ABCD     |5.5000000…|          10|
+---------+---------+----------+------------+
```

. To stop your streaming job, use the `DROP` statement to cancel it.
+
```sql
DROP JOB ingest_trades;
```

In the terminal where you started the Hazelcast member, you should see that the job is canceled as well as the time it was started and how long it ran for.

```
Start time: 2021-05-13T16:31:14.410
Duration: 00:02:48.318
```

== Next Steps

- xref:sql-overview.adoc[Learn about SQL]
- xref:pipelines:configuring-jobs.adoc[]
- xref:pipelines:job-management.adoc[]
- xref:sql-statements.adoc#job-management[SQL statements for job management]