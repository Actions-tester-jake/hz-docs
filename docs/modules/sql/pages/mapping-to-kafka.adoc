= Mapping to Kafka Topics
:description: Before you can query streaming data in Kafka topics, you need to create a mapping with the Kafka connector so that the SQL service knows how to access the topics in the most efficient way.
:page-aliases: sql:kafka-connector.adoc

{description}

== What is the Kafka Connector

The Kafka connector allows you to create streaming queries with SQL that continuously query a given Kafka topic.

Apache Kafka is a popular distributed, persistent log store which is a
great fit for stream processing systems. Data in Kafka is structured
as _topics_ and each topic consists of one or more partitions, stored in
the Kafka cluster.

== Installing the Connector

This connector is included in the full distribution of Hazelcast.

If you're using the slim distribution, you must add the link:https://mvnrepository.com/artifact/com.hazelcast.jet/hazelcast-jet-kafka/{page-component-version}[`hazelcast-jet-kafka` module] to your member's classpath.

NOTE: This connector is not supported on Solaris operating systems.

== Kafka Security

If you use Hazelcast Enterprise, your clients may need permissions to use this connector. For details, see xref:pipelines:job-security.adoc[].

== Configuration Options

To create a mapping, you must tell Hazelcast how to serialize and deserialize the topic keys and topic messages.

Although Apache Kafka is schema-less, SQL assumes a schema in which all
messages in a topic are of the same type (with some exceptions).

As well as <<primitives, primitives>>, Hazelcast supports the following formats:

* <<avro, Avro>>
* <<json, JSON>>
* <<java, Java>>

Topic keys and values can be in different formats. Any options not recognized by
Hazelcast are passed directly to the Kafka producer or consumer.

=== Primitives

If you want to use one of the xref:sql:data-types.adoc[primitive types] (any supported SQL data
type except `OBJECT`), you don't need to tell Hazelcast how to serialize or deserialize the data. Instead, Hazelcast will use the Java class
that represents the primitive type.

For example, if the topic key is an integer and the topic message is a
string, use:

```sql
CREATE MAPPING my_topic
TYPE Kafka
OPTIONS (
    'keyFormat'='int',
    'valueFormat'='varchar',
    'bootstrap.servers' = '127.0.0.1:9092'
);
```

In this case, the following options will be used automatically:

```sql
'key.serializer' = 'org.apache.kafka.common.serialization.IntegerSerializer',
'key.deserializer' = 'org.apache.kafka.common.serialization.IntegerDeserializer',
'value.serializer' = 'org.apache.kafka.common.serialization.StringSerializer',
'value.deserializer' = 'org.apache.kafka.common.serialization.StringDeserializer'
```

=== Avro

When using Avro, Hazelcast reads fields from the `GenericRecord` objects that are returned
by the `KafkaAvroDeserializer` instance. When inserting data into a topic, Hazelcast creates an
ad-hoc Avro schema named `jet.sql` from the mapping columns. As a result, you must provide a column list to the mapping.

Hazelcast currently can't use custom
Avro schemas to create objects, but it can use these schemas to read objects written
through our ad-hoc schema as long as the field names and types match.

[cols="m,m"]
|===
| SQL Type | Avro Type

a|`TINYINT`, `SMALLINT`, `INT`
|INT

|BIGINT
|LONG

|REAL
|FLOAT

|DOUBLE
|DOUBLE

|BOOLEAN
|BOOLEAN

a|`VARCHAR` and all other types
|STRING

|===

All Avro types are a union of the `NULL` type and the actual type.

```sql
CREATE MAPPING my_topic (
    __key VARCHAR,
    ticker VARCHAR,
    amount BIGINT,
    price DECIMAL
)
TYPE Kafka
OPTIONS (
    'keyFormat' = 'varchar',
    'valueFormat' = 'avro',
    'bootstrap.servers' = '127.0.0.1:9092',
    'schema.registry.url' = 'http://127.0.0.1:8081/'
    /* more Kafka options ... */
);
```

In this example, the key is a plain `Long` number, the value is serialized in the Avro format. `keyFormat` and `valueFormat` options are handled by
Hazelcast, the rest is passed directly to Kafka producer or consumer.

In this case
`io.confluent.kafka.serializers.KafkaAvroSerializer` and
`io.confluent.kafka.serializers.KafkaAvroDeserializer` are automatically
added to allow Hazelcast to serialize and deserialze the Avro data.

=== JSON

The value will be stored as a JSON object. Hazelcast can't automatically
determine the column list for this format, you must explicitly specify
it:

```sql
CREATE MAPPING my_topic(
    __key BIGINT,
    ticker VARCHAR,
    amount INT)
TYPE Kafka
OPTIONS (
    'keyFormat' = 'bigint',
    'valueFormat' = 'json',
    'bootstrap.servers' = '127.0.0.1:9092');
```

There are no additional options for this format.

JSON's type system doesn't match SQL's exactly. For example, JSON
numbers have unlimited precision, but such numbers are typically not
portable. We convert SQL integer and floating-point types into JSON
numbers. We convert the `DECIMAL` type, as well as all temporal types,
to JSON strings.

We don't support the `JSON` type from the SQL standard yet. That means
you can't use functions like `JSON_VALUE` or `JSON_QUERY`. If your JSON
documents don't all have the same fields or if they contain nested
objects, the usability is limited.

=== Java

Java serialization uses the
Java objects exactly as `KafkaConsumer.poll()` returns them. You can use
this option for objects serialized using Java serialization or any other
serialization method.

For this format you must specify the class name using `keyJavaClass` and
`valueJavaClass` options, for example:

```sql
CREATE MAPPING my_topic
TYPE Kafka
OPTIONS (
    'keyFormat' = 'java',
    'keyJavaClass' = 'java.lang.Long',
    'valueFormat' = 'java',
    'valueJavaClass' = 'com.example.Person',
    'value.serializer' = 'com.example.serialization.PersonSerializer',
    'value.deserializer' = 'com.example.serialization.PersonDeserializer',
    'bootstrap.servers' = '127.0.0.1:9092');
```

If the Java class corresponds to one of the basic data types (numbers,
dates, strings), that type will directly be used for the key or value
and mapped as a column named `__key` for keys and `this` for values. In
the example above, the key will be mapped with the `BIGINT` type. In
fact, the above `keyFormat` and `keyJavaClass` options are equivalent to
`'keyFormat'='bigint'`.

If the Java class is not one of the basic types, Hazelcast will analyze
the class using reflection and use its properties as column names. It
recognizes public fields and JavaBean-style getters. If some property
has a non-primitive type, it will be mapped under the `OBJECT` type.

=== External Column Names

You rarely need to specify the columns in DDL. If you do, you might need
to specify the external name for the column.

The entries in a map naturally have _key_ and _value_ elements. Because
of this, the format of the external name must be either `__key.<name>`
for a field in the key or `this.<name>` for a field in the value.

The external name defaults to `this.<columnName>`, so normally you only
need to specify it for key fields. There are also columns that represent
the entire key and value objects, called `__key` and `this`.