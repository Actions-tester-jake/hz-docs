= Map Connector
:description: A map is a distributed in-memory key-value data structure that can be used as a batch or streaming data source as well as a data sink.
:page-aliases: sql:imap-connector.adoc

A xref:data-structures:map.adoc[map] is a distributed in-memory key-value data structure that can be used as a batch or streaming data source as well as a data sink.

NOTE: With SQL, you cannot yet use maps as a streaming source or access them when they are stored in remote clusters.

== Installing the Connector

This connector is included in the full and slim
distributions of Hazelcast.

== Permissions

If you use Hazelcast Enterprise, you can set up permissions to restrict clients' access to these data structures.

For example, to read from map sources, you must add the `create` and `read` permissions for those maps. If you use the map connector to write to map sinks, you must add the `create` and `put` permissions for those maps.

For details, see xref:security:native-client-security.adoc[].

== SQL Configuration Options

To use the map connector in SQL, you must tell Hazelcast how to serialize/deserialize the keys and values, using the `keyFormat` and `valueFormat` options of the xref:sql:create-mapping.adoc[`CREATE MAPPING` statement].

As well as <<primitives, primitives>>, you can also serialize objects into the following formats:

* `portable`
* `json`
* `java` (Java serialization, `DataSerializable` or `IdentifiedDataSerializable`)

The key and value formats can be different.

=== Primitives

If you want to store one of the xref:sql:data-types.adoc[primitive types] (any supported SQL data
type except `OBJECT`) in a map entry, the Java class
representing that type will be stored in the map.

For example, to create a mapping for `IMap<Integer, String>`, use:

```sql
CREATE MAPPING my_map
TYPE IMap
OPTIONS (
    'keyFormat'='int',
    'valueFormat'='varchar'
)
```

=== Portable Objects

The benefit of xref:serialization:implementing-portable-serialization.adoc[`Portable` serialization] is that it doesn't deserialize the whole key
or value when reading only a subset of fields. Also it doesn't require a
custom Java class to be defined on the cluster, so it's usable for
non-Java clients.

To use this format, you need to specify additional options:

* `keyPortableFactoryId`, `valuePortableFactoryId`
* `keyPortableClassId`, `valuePortableClassId`
* `keyPortableVersion`, `valuePortableVersion` (optional, default is `0`)

If you omit a column list from the `CREATE MAPPING` command, Hazelcast will
resolve column names and types by looking at the `ClassDefinition`
found using the given factory ID, class ID, and version.

If the `ClassDefinition` with the given IDs is not known to the cluster,
you must provide a column list so that Hazelcast can use it to create the `ClassDefinition`.

Example mapping where both key and value are `Portable`:

```sql
CREATE MAPPING my_map
TYPE IMap
OPTIONS (
    'keyFormat' = 'portable',
    'keyPortableFactoryId' = '123',
    'keyPortableClassId' = '456',
    'keyPortableVersion' = '0',  -- optional
    'valueFormat' = 'portable',
    'valuePortableFactoryId' = '123',
    'valuePortableClassId' = '789',
    'valuePortableVersion' = '0'  -- optional
)
```

For more information on `Portable` see xref:serialization:implementing-portable-serialization.adoc[].

=== JSON Objects

To store values in JSON, you must declare the field names. For example, here, we create a mapping to a distributed map that stores a JSON object with the `ticker` and `amount` fields.

```sql
CREATE MAPPING my_map(
    __key BIGINT,
    ticker VARCHAR,
    amount INT)
TYPE IMap
OPTIONS (
    'keyFormat' = 'bigint',
    'valueFormat' = 'json')
```

There are no additional options for this format.

By default, Hazelcast serializes JSON into `HazelcastJsonValue` objects, which allows you to query its fields.

JSON's type system doesn't match SQL's exactly. For example, JSON
numbers have unlimited precision, but such numbers are typically not
portable. We convert SQL integer and floating-point types into JSON
numbers. We convert the `DECIMAL` type, as well as all temporal types,
to JSON strings.

NOTE: We don't yet support the `JSON` type from the SQL standard. That means
you can't use functions like `JSON_VALUE` or `JSON_QUERY`. If your JSON
documents don't all have the same fields or if they contain nested
objects, the usability is limited.

=== Java Objects

Java serialization uses the
Java object exactly as `map.get()` returns it. You can use it for
objects serialized using the Java serialization or Hazelcast custom
serialization (`DataSerializable` or `IdentifiedDataSerializable`).

You must specify the name of the Java class into which you want to serialize data, using the `keyJavaClass` and
`valueJavaClass` options. For example:

```sql
CREATE MAPPING my_map
TYPE IMap
OPTIONS (
    'keyFormat' = 'java',
    'keyJavaClass' = 'java.lang.Long',
    'valueFormat' = 'java',
    'valueJavaClass' = 'com.example.Person')
```

If the Java class corresponds to one of the basic data types (numbers,
dates, strings), that type will directly be used for the key or value
and mapped as a column named `__key` for keys and `this` for values. In
the example above, the key will be mapped with the `BIGINT` type. In
fact, the above `keyFormat` and `keyJavaClass` duo is equivalent to
`'keyFormat'='bigint'`.

If the Java class is not one of the basic types:

- Hazelcast will analyze the class using reflection and use its properties as column names.
+
Hazelcast recognizes public fields and JavaBean-style getters. If some property has a non-primitive type, it will be mapped under the `OBJECT` type.
- The class must be available to the cluster.
+
You can either add the class to the
members' classpaths by creating a JAR file and adding it to the `lib`
folder, or you can use user code deployment. User code deployment
must be enabled on the members, see xref:clusters:deploying-code-from-clients.adoc[] for details.

== Map as a Batch Source

Use maps as a batch source to query their keys and values or use them as part of a data pipeline.

As a batch data source, it's very easy to use without the need for any other
configuration:

```java
IMap<String, User> userCache = hz.getMap("usersCache")
Pipeline p = Pipeline.create();
p.readFrom(Sources.map(userCache));
 .writeTo(Sinks.logger()));
```

== Map as a Streaming Source

Maps can also be used as a streaming data source by enabling the xref:data-structures:event-journal.adoc[event journal].

Streaming data from maps is fault tolerant and supports exactly-once
processing.

The journal for a map is by default not enabled,
but can be enabled with the following configuration option:

[tabs] 
==== 
XML:: 
+ 
-- 
[source,xml]
----
<hazelcast>
    ...
    <map name="default">
        <event-journal enabled="true">
            <capacity>5000</capacity>
            <time-to-live-seconds>20</time-to-live-seconds>
        </event-journal>
    </map>
    ...
    <cache name="default">
        <event-journal enabled="true">
            <capacity>10000</capacity>
            <time-to-live-seconds>0</time-to-live-seconds>
        </event-journal>
    </cache>
    ...
</hazelcast>
----
--

YAML::
+
[source,yaml]
----
hazelcast:
  map:
    default:
      event-journal:
        enabled: true
        capacity: 5000
        time-to-live-seconds: 20
  cache:
    default:
      event-journal:
        enabled: true
        capacity: 10000
        time-to-live-seconds: 0
----

Java::
+
[source,java]
----
include::ROOT:example$/dds/EventJournalConfiguration.java[tag=ejc]
----
====

We can then modify the previous pipeline to instead stream the changes:


[tabs] 
==== 
Jet:: 
+ 
--
```java
IMap<String, User> userCache = hz.getMap("usersCache")
Pipeline p = Pipeline.create();
p.readFrom(Sources.mapJournal(userCache, START_FROM_OLDEST))
 .withIngestionTimestamps()
 .writeTo(Sinks.logger()));
```
--
SQL::
+
--
You cannot yet use maps as a streaming source in SQL or access maps in remote clusters.
--
====

By default, the source will emit only `ADDED` or `UPDATED` events and
the emitted object will have the key and the new value. You can change
to listen for all events by adding additional parameters to the source.

The capacity of the event journal is also an important consideration, as
having too little capacity will cause events to be dropped. Consider
also that the capacity is for all partitions and not shared per partition.
For example, if there are many updates to just one key, with the default
partition count of `271` and journal size of `100,000` the journal only
has space for `370` events per partitions.

For a tutorial, see the xref:pipelines:stream-imap.adoc[].

== Map as a Sink

By default, the map sink expects items of type `Entry<Key, Value>` and
will simply replace the previous entries, if any. However, there are
variants of the map sink that allow you to do atomic updates to existing
entries in the map by making use of `EntryProcessor` objects.

The updating sinks come in three variants:

- `mapWithMerging`, where you provide a function that computes the map value from the stream item and a merging function that gets called
only if a value already exists in the map. This is similar to the way
standard `Map.merge` method behaves. Hereâ€™s an example that
concatenates String values:
+
```java
Pipeline p = Pipeline.create();
p.readFrom(Sources.<String, User>map("userCache"))
 .map(user -> entry(user.country(), user))
 .writeTo(Sinks.mapWithMerging("usersByCountry",
    e -> e.getKey(),
    e -> e.getValue().name(),
    (oldValue, newValue) -> oldValue + ", " + newValue)
  );
```

- `mapWithUpdating`, where you provide a single updating function that
always gets called. It will be called on the stream item and the
existing value, if any. This can be used to add details to an
existing object for example. This is similar to the way standard
`Map.compute` method behaves. Here's an example that only updates a
field:
+
[source,java]
----
Pipeline p = Pipeline.create();
p.readFrom(Sources.<String, User>map("userCacheDetails"))
 .writeTo(Sinks.mapWithUpdating("userCache",
    e -> e.getKey(),
    (oldValue, entry) -> (oldValue != null ? oldValue.setDetails(entry.getValue) : null)
  ))
----

- `mapWithEntryProcessor`, where you provide a function that returns a
full-blown `EntryProcessor` instance that will be submitted to the
map. This is the most general variant. This example takes the
values of the map and submits an entry processor that increments the
values by 5:
+
```java
Pipeline p = Pipeline.create();
p.readFrom(Sources.<String, Integer>map("input"))
 .writeTo(Sinks.mapWithEntryProcessor("output",
    entry -> entry.getKey(),
    entry -> new IncrementEntryProcessor())
  );

static class IncrementEntryProcessor implements EntryProcessor<String, Integer, Integer> {
    @Override
    public Integer process(Entry<String, Integer> entry) {
        return entry.setValue(entry.getValue() + 5);
    }
}
```

The variants above can be used to remove existing map entries by
setting their values to `null`. To put it another way, if these map sink
variants set the entryâ€™s value to null, the entry will be removed
from the map.

== Predicates and Projections

If your use case calls for some filtering and/or transformation of the
data you retrieve, you can optimize the pipeline by providing a
filtering predicate and an arbitrary transformation function to the
source connector itself and theyâ€™ll get applied before the data is
processed by Hazelcast. This can be advantageous especially in the cases when
the data source is in another cluster. See the example below:

```java
IMap<String, Person> personCache = jet.getMap("personCache");
Pipeline p = Pipeline.create();
p.readFrom(Sources.map(personCache,
    Predicates.greaterEqual("age", 21),
    Projections.singleAttribute("name"))
);
```